// UBViews.Query.Lexer
// fslex --unicode QueryLexer.fsl

{
module UBViews.Query.Lexer

open System
open UBViews.Query.Parser

open FSharp.Text.Lexing

}
 
// These are some regular expression definitions
let digit = ['0'-'9']
let whitespace = [' ' '\t' ]
let newline = ('\n' | '\r' '\n')

// Just grab a sequence of characters to form a token. We will turn that into
// a 'document token' later.
let wordcharacter = ['a'-'z''A'-'Z''\'']
let softcharacter = ['.' ',' '!' '?']
let term = (wordcharacter | softcharacter)+

rule tokenize = parse
// Eat whitespace
| whitespace	{ tokenize lexbuf }
| newline       { tokenize lexbuf }
// Symbols
| "\""			{ QUOTATION_MARK }
| "("			{ LEFT_PAREN }
| ")"			{ RIGHT_PAREN }
// Keywords
| "AND"
| "and"			{ AND }
| "OR"
| "or"			{ OR }
// Additional Keywords
| "FILTERBY"
| "filterby"	{ FILTERBY }
| "TOPID"
| "topid"		{ FILTERID(LexBuffer<_>.LexemeString(lexbuf)) }
| "DOCID"
| "docid"		{ FILTERID(LexBuffer<_>.LexemeString(lexbuf)) }
| "SEQID"
| "seqid"		{ FILTERID(LexBuffer<_>.LexemeString(lexbuf)) }
| "PARID"
| "parid"		{ FILTERID(LexBuffer<_>.LexemeString(lexbuf)) }
| "SECID"
| "secid"		{ FILTERID(LexBuffer<_>.LexemeString(lexbuf)) }
// Any other string is considered a term in a search query.
| term         { TERM(LexBuffer<_>.LexemeString(lexbuf)) }
// EOF
| eof   { EOF }